#!/usr/bin/env python3
"""
FABRIC MCP Server

This module implements a Model Context Protocol (MCP) server that exposes FABRIC testbed
API operations as LLM-accessible tools. It provides topology queries, slice management,
and resource operations through a unified FastMCP interface.

Key Features:
- Background caching of topology data (sites, hosts, facility ports, links)
- Bearer token authentication for secure API access
- Structured log_helper with request tracing
- Async tool execution with performance monitoring
"""
from __future__ import annotations

import os
import sys
import time
import uuid
import asyncio
import logging
from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple, Callable

from fastmcp import FastMCP
from fastmcp.server.context import Context
from fastmcp.server.dependencies import get_http_headers

from fim.user import GraphFormat

from fabrictestbed.fabric_manager_v2 import FabricManagerV2
from functools import wraps

# ---------------------------------------
# Config (env with sensible defaults)
# ---------------------------------------
# FABRIC service endpoints - can be overridden via environment variables
# for different deployments (production, staging, development)
FABRIC_ORCHESTRATOR_HOST = os.environ.get("FABRIC_ORCHESTRATOR_HOST", "orchestrator.fabric-testbed.net")
FABRIC_CREDMGR_HOST      = os.environ.get("FABRIC_CREDMGR_HOST", "cm.fabric-testbed.net")
FABRIC_AM_HOST           = os.environ.get("FABRIC_AM_HOST", "artifacts.fabric-testbed.net")
FABRIC_CORE_API_HOST     = os.environ.get("FABRIC_CORE_API_HOST", "uis.fabric-testbed.net")

# Print configuration on startup for debugging/verification
print(f"Orchestrator HOST: {FABRIC_ORCHESTRATOR_HOST}")
print(f"Credmgr HOST: {FABRIC_CREDMGR_HOST}")
print(f"Artifact Manager HOST: {FABRIC_AM_HOST}")
print(f"Core API HOST: {FABRIC_CORE_API_HOST}")

# ---------------------------------------
# Logging setup
# ---------------------------------------
# Configurable log_helper: supports both human-readable text and machine-parseable JSON formats
LOG_LEVEL  = os.environ.get("LOG_LEVEL", "INFO").upper()
LOG_FORMAT = os.environ.get("LOG_FORMAT", "text").lower()  # "text" | "json"
UVICORN_ACCESS_LOG = os.environ.get("UVICORN_ACCESS_LOG", "1") not in ("0", "false", "False")

class _JsonFormatter(logging.Formatter):
    """
    Custom JSON formatter for structured log_helper.

    Outputs log records as JSON objects with standard fields (timestamp, level, logger, message)
    plus any extra context fields like request_id, tool name, duration, etc.
    """
    def format(self, record: logging.LogRecord) -> str:
        base = {
            "ts": self.formatTime(record, datefmt="%Y-%m-%dT%H:%M:%S%z"),
            "level": record.levelname,
            "logger": record.name,
            "msg": record.getMessage(),
        }
        # Include extra fields if present (for request tracing and performance metrics)
        for k in ("request_id", "tool", "path", "method", "status", "duration_ms", "client"):
            if hasattr(record, k):
                base[k] = getattr(record, k)
        if record.exc_info:
            base["exc_info"] = self.formatException(record.exc_info)
        return __import__("json").dumps(base, ensure_ascii=False)

def configure_logging() -> None:
    """
    Configure root logger with either text or JSON formatting.

    Sets up a StreamHandler to stdout with the appropriate formatter based on LOG_FORMAT.
    Also configures uvicorn and fastapi loggers to use the same settings.
    """
    root = logging.getLogger()
    root.setLevel(LOG_LEVEL)

    # Clean existing handlers (important when reloading to avoid duplicate logs)
    for h in list(root.handlers):
        root.removeHandler(h)

    handler = logging.StreamHandler(sys.stdout)
    if LOG_FORMAT == "json":
        fmt = _JsonFormatter()
    else:
        fmt = logging.Formatter(
            fmt="%(asctime)s %(levelname)s [%(name)s] %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )
    handler.setFormatter(fmt)
    root.addHandler(handler)

    # Align common library loggers with our configuration
    for name in ("uvicorn", "uvicorn.error", "uvicorn.access", "fastapi"):
        logging.getLogger(name).setLevel(LOG_LEVEL)
        logging.getLogger(name).propagate = True

configure_logging()
log = logging.getLogger("fabric.mcp")

# ---------------------------------------
# MCP App
# ---------------------------------------
# Initialize FastMCP server with metadata
mcp = FastMCP(
    name="fabric-mcp-proxy",
    instructions="Proxy for accessing FABRIC API data via LLM tool calls.",
    version="2.0.0",
)

# ---------------------------------------
# Access log middleware (FastAPI)
# ---------------------------------------
# Adds HTTP request/response log_helper with request ID tracing for debugging
if hasattr(mcp, "app") and mcp.app:
    from fastapi import Request
    @mcp.app.middleware("http")
    async def access_log_middleware(request: Request, call_next):
        # Generate or extract request ID for tracing through the system
        rid = request.headers.get("x-request-id") or uuid.uuid4().hex[:12]
        start = time.perf_counter()
        try:
            response = await call_next(request)
            status = getattr(response, "status_code", 0)
        except Exception:
            status = 500
            log.exception("Unhandled exception during request",
                          extra={"request_id": rid, "path": request.url.path, "method": request.method})
            raise
        finally:
            # Log request completion with timing information
            dur_ms = round((time.perf_counter() - start) * 1000, 2)
            if UVICORN_ACCESS_LOG:
                log.info("HTTP %s %s -> %s in %.2fms",
                         request.method, request.url.path, status, dur_ms,
                         extra={
                             "request_id": rid,
                             "path": request.url.path,
                             "method": request.method,
                             "status": status,
                             "duration_ms": dur_ms,
                             "client": request.client.host if request.client else None,
                         })
        # Return request_id in response headers for client-side tracing
        response.headers["x-request-id"] = rid
        return response

# ---------------------------------------
# Tool log_helper decorator
# ---------------------------------------
def tool_logger(tool_name: str):
    """
    Decorator that wraps MCP tool functions with log_helper and timing.

    Logs tool invocation start/end with request IDs for tracing, execution duration,
    and result size. Helps track performance and debug issues.

    Args:
        tool_name: Name of the tool being wrapped (for log messages)

    Returns:
        Decorator function that wraps async tool functions
    """
    def _wrap(fn):
        @wraps(fn)  # preserves __name__, __doc__, annotations for FastMCP
        async def _async_wrapper(*args, **kwargs):
            # Extract request ID from context or tool call parameters for tracing
            ctx = args[0] if args else None
            rid = None
            try:
                if ctx and hasattr(ctx, "request") and ctx.request:
                    rid = ctx.request.headers.get("x-request-id")
            except Exception:
                pass
            rid = rid or kwargs.get("toolCallId") or kwargs.get("tool_call_id") or uuid.uuid4().hex[:12]

            # Log tool start and measure execution time
            start = time.perf_counter()
            log.info("Tool start", extra={"tool": tool_name, "request_id": rid})
            try:
                result = await fn(*args, **kwargs)
                dur_ms = round((time.perf_counter() - start) * 1000, 2)
                # Track result size for performance analysis
                size = None
                if isinstance(result, list):
                    size = len(result)
                elif isinstance(result, dict):
                    size = result.get("count") or len(result)
                log.info("Tool done in %.2fms (size=%s)", dur_ms, size,
                         extra={"tool": tool_name, "request_id": rid, "duration_ms": dur_ms})
                return result
            except Exception:
                # Log errors with timing for debugging
                dur_ms = round((time.perf_counter() - start) * 1000, 2)
                log.exception("Tool error after %.2fms",
                              extra={"tool": tool_name, "request_id": rid, "duration_ms": dur_ms})
                raise
        return _async_wrapper
    return _wrap

# ---------------------------------------
# Background Resource Cache
# ---------------------------------------
# Periodically refreshes topology data (sites, hosts, facility ports, links)
# to reduce API latency for read-heavy operations
from resources_cache import ResourceCache

REFRESH_INTERVAL = int(os.environ.get("REFRESH_INTERVAL_SECONDS", "300"))  # 5 minutes default
CACHE_MAX_FETCH  = int(os.environ.get("CACHE_MAX_FETCH", "5000"))  # Max items per refresh
CACHE = ResourceCache(interval_seconds=REFRESH_INTERVAL, max_fetch=CACHE_MAX_FETCH)

def _fm_factory_for_cache():
    """
    Factory function to create FabricManagerV2 instances for cache refreshes.

    Returns a FabricManagerV2 configured for the cache. The cache will decide
    whether to use a stored token or make public (unauthenticated) calls.
    """
    return FabricManagerV2(
        credmgr_host=FABRIC_CREDMGR_HOST,
        orchestrator_host=FABRIC_ORCHESTRATOR_HOST,
        http_debug=bool(int(os.environ.get("HTTP_DEBUG", "0"))),
    )

async def _on_startup():
    """Start the background cache refresher on application startup."""
    log.info("Starting background cache refresher (interval=%ss, max_fetch=%s)",
             REFRESH_INTERVAL, CACHE_MAX_FETCH)
    CACHE.wire_fm_factory(_fm_factory_for_cache)
    await CACHE.start()

async def _on_shutdown():
    """Stop the background cache refresher on application shutdown."""
    log.info("Stopping background cache refresher")
    await CACHE.stop()

# Wire up lifecycle handlers
if hasattr(mcp, "app") and mcp.app:
    mcp.app.add_event_handler("startup", _on_startup)
    mcp.app.add_event_handler("shutdown", _on_shutdown)

# Load system prompt for LLM context
#SYSTEM_TEXT = Path("/app/system.md").read_text(encoding="utf-8").strip()

#@mcp.prompt(name="fabric-system")
#def fabric_system_prompt():
#    """MCP prompt that returns the FABRIC system instructions for LLMs."""
 #   return SYSTEM_TEXT

# ---------------------------------------
# Helpers
# ---------------------------------------
def _bearer_from_headers(headers: Dict[str, str]) -> Optional[str]:
    """
    Extract Bearer token from HTTP Authorization header.

    Args:
        headers: Dictionary of HTTP headers (case-insensitive)

    Returns:
        Token string if found, None otherwise
    """
    low = {k.lower(): v for k, v in headers.items()}
    auth = low.get("authorization", "").strip()
    if auth.lower().startswith("bearer "):
        return auth.split(" ", 1)[1].strip()
    return None

def _fabric_manager() -> Tuple[FabricManagerV2, str]:
    """
    Create a FabricManagerV2 instance authenticated with the request's Bearer token.

    Extracts the token from HTTP headers and validates it's present. This is used
    for all authenticated tool calls that need user-specific access.

    Returns:
        Tuple of (FabricManagerV2 instance, token string)

    Raises:
        ValueError: If Authorization header is missing or invalid
    """
    headers = get_http_headers() or {}
    token = _bearer_from_headers(headers)
    if not token:
        log.warning("Missing Authorization header on protected call")
        raise ValueError("Authentication Required: Missing or invalid Authorization Bearer token.")
    fm = FabricManagerV2(
        credmgr_host=FABRIC_CREDMGR_HOST,
        orchestrator_host=FABRIC_ORCHESTRATOR_HOST,
        http_debug=bool(int(os.environ.get("HTTP_DEBUG", "0"))),
    )
    # Optional: teach the cache about this token for future refreshes
    # Uncomment to enable token learning for cache:
    # try:
    #     loop = asyncio.get_running_loop()
    #     loop.create_task(CACHE.note_token(token))
    # except RuntimeError:
    #     pass
    return fm, token

async def _call_threadsafe(fn, **kwargs):
    """
    Execute a synchronous function in a thread pool to avoid blocking the event loop.

    Filters out None values from kwargs before passing to the function.

    Args:
        fn: Synchronous function to execute
        **kwargs: Keyword arguments to pass (None values filtered out)

    Returns:
        Result of the function call
    """
    return await asyncio.to_thread(fn, **{k: v for k, v in kwargs.items() if v is not None})

# ---------------------------------------
# Topology Query Helpers
# ---------------------------------------
def _apply_sort(items: List[Dict[str, Any]], sort: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Sort items by a specified field and direction.

    Args:
        items: List of dictionaries to sort
        sort: Sort specification with "field" and "direction" (asc/desc)

    Returns:
        Sorted list (items with None values for the field are placed last)
    """
    if not sort or not isinstance(sort, dict):
        return items
    field = sort.get("field")
    if not field:
        return items
    direction = (sort.get("direction") or "asc").lower()
    reverse = direction == "desc"
    # Sort with None values last, regardless of direction
    return sorted(items, key=lambda r: (r.get(field) is None, r.get(field)), reverse=reverse)

def _paginate(items: List[Dict[str, Any]], limit: Optional[int], offset: int) -> List[Dict[str, Any]]:
    """
    Apply pagination to a list of items.

    Args:
        items: List to paginate
        limit: Maximum number of items to return (None = all)
        offset: Number of items to skip from the start

    Returns:
        Paginated slice of the items list
    """
    start = max(0, int(offset or 0))
    if limit is None:
        return items[start:]
    return items[start : start + max(0, int(limit))]

# Maximum items to fetch when sorting is requested (avoids memory issues with large datasets)
_MAX_FETCH_FOR_SORT = int(os.environ.get("MAX_FETCH_FOR_SORT", "5000"))

# ---------------------------------------
# MCP Tools: Topology Queries
# ---------------------------------------
# These tools query FABRIC topology resources with caching support

@mcp.tool(
    name="query-sites",
    title="Query Sites",
    description=(
        "List sites with filters and optional sorting.\n"
        "filters: dict supporting operators eq, ne, lt, lte, gt, gte, in, contains, icontains, regex, any, all, and 'or'.\n"
        "sort: {\"field\": \"name|cores_available|...\", \"direction\": \"asc|desc\"}"
    ),
)
@tool_logger("query-sites")
async def query_sites(
    ctx: Context,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
    filters: Optional[Dict[str, Any]] = None,
    sort: Optional[Dict[str, Any]] = None,
    limit: Optional[int] = 200,
    offset: int = 0,
) -> List[Dict[str, Any]]:
    """Query FABRIC sites with optional filtering, sorting, and pagination."""
    # Try to use cached data first
    snap = CACHE.snapshot()
    items = list(snap.sites) if snap.sites else None
    if items is None:
        # Cache miss - fetch from API
        fm, id_token = _fabric_manager()
        fm_limit = _MAX_FETCH_FOR_SORT if sort else limit
        items = await _call_threadsafe(
            fm.query_sites, id_token=id_token, filters=filters, limit=fm_limit, offset=0
        )
    items = _apply_sort(items, sort)
    return _paginate(items, limit=limit, offset=offset)

@mcp.tool(
    name="query-hosts",
    title="Query Hosts",
    description=(
        "List hosts with filters and optional sorting. "
        "Filter on fields like site, name, cores_* / ram_* / disk_* or nested components (client-side contains/regex).\n"
        "filters: operator dicts; sort: {\"field\": \"cores_available\", \"direction\": \"desc\"}"
    ),
)
@tool_logger("query-hosts")
async def query_hosts(
    ctx: Context,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
    filters: Optional[Dict[str, Any]] = None,
    sort: Optional[Dict[str, Any]] = None,
    limit: Optional[int] = 200,
    offset: int = 0,
) -> List[Dict[str, Any]]:
    """Query FABRIC hosts with optional filtering, sorting, and pagination."""
    # Try cache first, fall back to API on cache miss
    snap = CACHE.snapshot()
    items = list(snap.hosts) if snap.hosts else None
    if items is None:
        fm, id_token = _fabric_manager()
        fm_limit = _MAX_FETCH_FOR_SORT if sort else limit
        items = await _call_threadsafe(
            fm.query_hosts, id_token=id_token, filters=filters, limit=fm_limit, offset=0
        )
    items = _apply_sort(items, sort)
    return _paginate(items, limit=limit, offset=offset)

@mcp.tool(
    name="query-facility-ports",
    title="Query Facility Ports",
    description=(
        "List facility ports with filters and optional sorting. "
        "Common fields: site, name, vlans, port, switch, labels.\n"
        "filters: operator dicts; sort optional."
    ),
)
@tool_logger("query-facility-ports")
async def query_facility_ports(
    ctx: Context,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
    filters: Optional[Dict[str, Any]] = None,
    sort: Optional[Dict[str, Any]] = None,
    limit: Optional[int] = 200,
    offset: int = 0,
) -> List[Dict[str, Any]]:
    """Query FABRIC facility ports with optional filtering, sorting, and pagination."""
    snap = CACHE.snapshot()
    items = list(snap.facility_ports) if snap.facility_ports else None
    if items is None:
        fm, id_token = _fabric_manager()
        fm_limit = _MAX_FETCH_FOR_SORT if sort else limit
        items = await _call_threadsafe(
            fm.query_facility_ports, id_token=id_token, filters=filters, limit=fm_limit, offset=0
        )
    items = _apply_sort(items, sort)
    return _paginate(items, limit=limit, offset=offset)

@mcp.tool(
    name="query-links",
    title="Query Links",
    description=(
        "List L2/L3 links with filters and optional sorting. "
        "Fields: name, layer, labels, bandwidth, endpoints (array of {site,node,port}).\n"
        "Example filter to touch a site: "
        "{\"or\": [{\"name\": {\"icontains\": \"UCSD\"}}, {\"layer\": {\"eq\": \"L2\"}}]}"
    ),
)
@tool_logger("query-links")
async def query_links(
    ctx: Context,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
    filters: Optional[Dict[str, Any]] = None,
    sort: Optional[Dict[str, Any]] = None,
    limit: Optional[int] = 200,
    offset: int = 0,
) -> List[Dict[str, Any]]:
    """Query FABRIC network links with optional filtering, sorting, and pagination."""
    snap = CACHE.snapshot()
    items = list(snap.links) if snap.links else None
    if items is None:
        fm, id_token = _fabric_manager()
        fm_limit = _MAX_FETCH_FOR_SORT if sort else limit
        items = await _call_threadsafe(
            fm.query_links, id_token=id_token, filters=filters, limit=fm_limit, offset=0
        )
    items = _apply_sort(items, sort)
    return _paginate(items, limit=limit, offset=offset)

# ---------------------------------------
# MCP Tools: Slice Management
# ---------------------------------------
# These tools manage FABRIC slices (user experiments/resources)
@mcp.tool
@tool_logger("query-slices")
async def query_slices(
    ctx: Context,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
    as_self: bool = True,
    slice_id: Optional[str] = None,
    slice_name: Optional[str] = None,
    slice_state: Optional[List[str]] = None,
    exclude_slice_state: Optional[List[str]] = None,
    offset: int = 0,
    limit: int = 200,
    fetch_all: bool = True,
    graph_format: Optional[str] = str(GraphFormat.GRAPHML),
) -> Dict[str, Any]:
    """
        List FABRIC slices with optional filtering.

        Args:
            slice_id: slice GUID
            slice_name: slice name
            slice_state: Optional list of slice states to include (e.g., ["StableError", "StableOK"]). Allowed values: (Nascent, Configuring, StableOK,
                     StableError, ModifyOK, ModifyError, Closing, Dead).
            exclude_slice_state: Optional list of slice states to exclude (e.g., for fetching active slices set exclude_states=["Closing", "Dead"]).
            as_self: If True, list only user's own slices; if False, list all accessible slices.
            limit: Maximum number of slices to return (default: 200).
            offset: Pagination offset (default: 0).
            fetch_all: If True, automatically fetch all pages

        Returns:
            Dictionary of slice data with slice name as the key.
    """
    fm, id_token = _fabric_manager()

    # Single slice lookup by ID
    if slice_id:
        item = await _call_threadsafe(
            fm.get_slice,
            id_token=id_token,
            slice_id=slice_id,
            graph_format="GRAPHML",
            as_self=as_self,
            return_fmt="dict",
        )
        key = item.get("name") or item.get("slice_id") or "slice"
        return {key: item}

    # List slices with optional pagination
    results: List[Dict[str, Any]] = []
    cur_offset = offset
    while True:
        page = await _call_threadsafe(
            fm.list_slices,
            id_token=id_token,
            states=slice_state,
            name=slice_name,
            search=None,
            exact_match=False,
            as_self=as_self,
            limit=limit,
            offset=cur_offset,
            return_fmt="dict",
        )
        if not page:
            break
        # Client-side filtering for excluded states
        if exclude_slice_state:
            exclude_set = set(exclude_slice_state)
            page = [p for p in page if (p.get("state") not in exclude_set)]
        results.extend(page)
        if not fetch_all or len(page) < limit:
            break
        cur_offset += limit

    # Build dict keyed by slice name (handle duplicates)
    out: Dict[str, Any] = {}
    for s in results:
        key = s.get("name") or s.get("slice_id")
        if key in out and s.get("slice_id"):
            key = f"{key}-{s['slice_id'][:8]}"
        out[key] = s
    return out

@mcp.tool
@tool_logger("get-slivers")
async def get_slivers(
    ctx: Context,
    slice_id: str,
    as_self: bool = True,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    List all slivers (resource allocations) in a slice.

    Args:
        slice_id: UUID of the slice containing the slivers.
        as_self: If True, list as owner; if False, list with delegated access.

    Returns:
        List of sliver dictionaries for sliver data.
    """
    fm, id_token = _fabric_manager()
    slivers = await _call_threadsafe(
        fm.list_slivers,
        id_token=id_token,
        slice_id=slice_id,
        as_self=as_self,
        return_fmt="dict",
    )
    return slivers

@mcp.tool
@tool_logger("create-slice")
async def create_slice(
    ctx: Context,
    name: str,
    graph_model: str,
    ssh_keys: List[str],
    lifetime: Optional[int] = None,
    lease_start_time: Optional[str] = None,
    lease_end_time: Optional[str] = None,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Create a new FABRIC slice.

    Args:
        name: Name of the slice to create.
        graph_model: Slice topology graph model (GRAPHML, JSON, etc.).
        ssh_keys: List of SSH public keys for slice access.
        lifetime: Optional slice lifetime in days.
        lease_start_time: Optional lease start time (UTC format).
        lease_end_time: Optional lease end time (UTC format).

    Returns:
        List of sliver dictionaries representing the created slice resources.
    """
    fm, id_token = _fabric_manager()
    slivers = await _call_threadsafe(
        fm.create_slice,
        id_token=id_token,
        name=name,
        graph_model=graph_model,
        ssh_keys=ssh_keys,
        lifetime=lifetime,
        lease_start_time=lease_start_time,
        lease_end_time=lease_end_time,
        return_fmt="dict",
    )
    return slivers

@mcp.tool
@tool_logger("modify-slice")
async def modify_slice(
    ctx: Context,
    slice_id: str,
    graph_model: str,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """
    Modify an existing FABRIC slice topology.

    Args:
        slice_id: UUID of the slice to modify.
        graph_model: Updated slice topology graph model.

    Returns:
        List of sliver dictionaries with modification results.
    """
    fm, id_token = _fabric_manager()
    slivers = await _call_threadsafe(
        fm.modify_slice,
        id_token=id_token,
        slice_id=slice_id,
        graph_model=graph_model,
        return_fmt="dict",
    )
    return slivers

@mcp.tool
@tool_logger("accept-modify")
async def accept_modify(
    ctx: Context,
    slice_id: str,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Accept pending slice modifications.

    Args:
        slice_id: UUID of the slice with pending modifications.

    Returns:
        Slice dictionary with updated state.
    """
    fm, id_token = _fabric_manager()
    accepted = await _call_threadsafe(
        fm.accept_modify,
        id_token=id_token,
        slice_id=slice_id,
        return_fmt="dict",
    )
    return accepted

@mcp.tool(
    name="renew-slice",
    title="Renew Slice",
    description="Extend a slice to a new lease_end_time (format: 'YYYY-MM-DD HH:MM:SS +0000')."
)
@tool_logger("renew-slice")
async def renew_slice(
    ctx: Context,
    slice_id: str,
    lease_end_time: str,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Renew a FABRIC slice lease.

    Args:
        slice_id: UUID of the slice to renew.
        lease_end_time: New lease end time (UTC format).
    """
    fm, id_token = _fabric_manager()
    await _call_threadsafe(
        fm.renew_slice,
        id_token=id_token,
        slice_id=slice_id,
        lease_end_time=lease_end_time,
    )
    return {"status": "ok", "slice_id": slice_id, "lease_end_time": lease_end_time}

@mcp.tool
@tool_logger("delete-slice")
async def delete_slice(
    ctx: Context,
    toolCallId: Optional[str] = None,
    tool_call_id: Optional[str] = None,
    slice_id: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Delete a FABRIC slice.

    Args:
        slice_id: Optional UUID of the slice to delete.
    """
    fm, id_token = _fabric_manager()
    await _call_threadsafe(
        fm.delete_slice,
        id_token=id_token,
        slice_id=slice_id,
    )
    return {"status": "ok", "slice_id": slice_id}

# ---------------------------------------
# Server Entry Point
# ---------------------------------------
if __name__ == "__main__":
    # Configure server host and port from environment
    port = int(os.getenv("PORT", "5000"))
    host = os.getenv("HOST", "0.0.0.0")
    if UVICORN_ACCESS_LOG:
        os.environ.setdefault("UVICORN_ACCESS_LOG", "true")
    log.info("Starting FABRIC MCP (FastMCP) on http://%s:%s", host, port)
    # Run the MCP server with HTTP transport
    mcp.run(transport="http", host=host, port=port)
